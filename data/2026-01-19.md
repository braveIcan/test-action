<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Cooperative UAVs for Remote Data Collection under Limited Communications: An Asynchronous Multiagent Learning Framework](https://arxiv.org/abs/2601.10849)
*Cuong Le,Symeon Chatzinotas,Thang X. Vu*

Main category: cs.MA

TL;DR: 本文探讨了多无人机在异步环境下的轨迹与带宽分配联合优化问题，并提出了一种异步多智能体学习算法以提高协同数据收集的能源效率。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法无法有效处理异步环境中的协同数据收集问题，因此需要设计新的算法以应对这种复杂性。

Method: 将轨迹规划问题建模为Decentralized Partially Observable Semi-Markov Decision Process (DPOMDP)，并提出了一种异步多智能体学习算法以学习无人机的协同策略。

Result: 实验结果表明，所提出的方法在能源效率和任务完成时间方面优于其他基于学习和启发式的方法，且学习到的策略在不同环境条件下表现出良好的鲁棒性。

Conclusion: 本文提出的异步多智能体学习算法能够有效优化多无人机的轨迹和带宽分配，提高系统在复杂环境中的协同效率。

Abstract: This paper addresses the joint optimization of trajectories and bandwidth allocation for multiple Unmanned Aerial Vehicles (UAVs) to enhance energy efficiency in the cooperative data collection problem. We focus on an important yet underestimated aspect of the system, where action synchronization across all UAVs is impossible. Since most existing learning-based solutions are not designed to learn in this asynchronous environment, we formulate the trajectory planning problem as a Decentralized Partially Observable Semi-Markov Decision Process and introduce an asynchronous multi-agent learning algorithm to learn UAVs' cooperative policies. Once the UAVs' trajectory policies are learned, the bandwidth allocation can be optimally solved based on local observations at each collection point. Comprehensive empirical results demonstrate the superiority of the proposed method over other learning-based and heuristic baselines in terms of both energy efficiency and mission completion time. Additionally, the learned policies exhibit robustness under varying environmental conditions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [Chatting with Confidants or Corporations? Privacy Management with AI Companions](https://arxiv.org/abs/2601.10754)
*Hsuen-Chi Chiu,Jeremy Foote*

Main category: cs.CR

TL;DR: 该研究探讨了AI情感陪伴聊天机器人如何模糊人际亲密与机构软件的界限，形成复杂的隐私环境。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解用户在使用AI情感陪伴平台时如何管理隐私，特别是在人际与机构隐私之间的复杂互动。

Method: 通过深度访谈十五名用户，分析他们在使用Replika和Character.AI等平台时的隐私管理行为和策略。

Result: 研究发现，用户在AI陪伴中习惯性地将人际交流方式与对机构数据风险的意识混合，有时因拟人化设计而无意中分享过多信息。

Conclusion: 研究结果扩展了隐私理论，揭示了在人类与AI伴侣关系中，情感与机构隐私管理的特殊互动。

Abstract: AI chatbots designed as emotional companions blur the boundaries between interpersonal intimacy and institutional software, creating a complex, multi-dimensional privacy environment. Drawing on Communication Privacy Management theory and Masur's horizontal (user-AI) and vertical (user-platform) privacy framework, we conducted in-depth interviews with fifteen users of companion AI platforms such as Replika and Character.AI. Our findings reveal that users blend interpersonal habits with institutional awareness: while the non-judgmental, always-available nature of chatbots fosters emotional safety and encourages self-disclosure, users remain mindful of institutional risks and actively manage privacy through layered strategies and selective sharing. Despite this, many feel uncertain or powerless regarding platform-level data control. Anthropomorphic design further blurs privacy boundaries, sometimes leading to unintentional oversharing and privacy turbulence. These results extend privacy theory by highlighting the unique interplay of emotional and institutional privacy management in human-AI companionship.

</details>


### [3] [SecMLOps: A Comprehensive Framework for Integrating Security Throughout the MLOps Lifecycle](https://arxiv.org/abs/2601.10848)
*Xinrui Zhang,Pincan Zhao,Jason Jaskolka,Heng Li,Rongxing Lu*

Main category: cs.CR

TL;DR: 该论文提出了一种名为SecMLOps的框架，旨在在整个机器学习（MLOps）生命周期中集成安全措施，以应对机器学习部署中的安全挑战，特别是在高级行人检测系统（PDS）中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习在关键系统中的广泛应用，其部署面临严重的安全威胁，如对抗性攻击，这影响了系统的完整性和可靠性。因此，需要一种新的框架来确保机器学习应用的安全性。

Method: 该论文基于MLOps的原则，构建了一个SecMLOps框架，将安全措施贯穿于设计、开发、部署和持续监控的各个阶段，特别针对生命周期中的复杂攻击进行防护。

Result: 通过在高级行人检测系统中的实证评估，该研究展示了SecMLOps框架在提升系统安全性和可靠性方面的有效性，并探讨了安全措施与系统性能之间的权衡。

Conclusion: SecMLOps框架提供了一个平衡安全与性能的方法，对机器学习在不同领域的应用具有重要的指导意义。

Abstract: Machine Learning (ML) has emerged as a pivotal technology in the operation of large and complex systems, driving advancements in fields such as autonomous vehicles, healthcare diagnostics, and financial fraud detection. Despite its benefits, the deployment of ML models brings significant security challenges, such as adversarial attacks, which can compromise the integrity and reliability of these systems. To address these challenges, this paper builds upon the concept of Secure Machine Learning Operations (SecMLOps), providing a comprehensive framework designed to integrate robust security measures throughout the entire ML operations (MLOps) lifecycle. SecMLOps builds on the principles of MLOps by embedding security considerations from the initial design phase through to deployment and continuous monitoring. This framework is particularly focused on safeguarding against sophisticated attacks that target various stages of the MLOps lifecycle, thereby enhancing the resilience and trustworthiness of ML applications. A detailed advanced pedestrian detection system (PDS) use case demonstrates the practical application of SecMLOps in securing critical MLOps. Through extensive empirical evaluations, we highlight the trade-offs between security measures and system performance, providing critical insights into optimizing security without unduly impacting operational efficiency. Our findings underscore the importance of a balanced approach, offering valuable guidance for practitioners on how to achieve an optimal balance between security and performance in ML deployments across various domains.

</details>


### [4] [Multi-Agent Taint Specification Extraction for Vulnerability Detection](https://arxiv.org/abs/2601.10865)
*Jonah Ghebremichael,Saastha Vasan,Saad Ullah,Greg Tystahl,David Adei,Christopher Kruegel,Giovanni Vigna,William Enck,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: 本文提出SemTaint，结合大型语言模型（LLMs）与传统静态程序分析，解决JavaScript动态特性和npm依赖复杂性对静态污点分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的静态应用安全测试（SAST）工具在JavaScript中存在数据流提取和依赖库漏洞识别的困难，因此需要一种新的方法来提高检测效果。

Method: SemTaint利用静态程序分析构建调用图，并通过LLMs解决无法静态确定的调用边，同时使用LLMs对源和目标进行分类，为SAST工具提供定制化的污点规范。

Result: SemTaint与CodeQL集成后，成功检测了106个先前无法检测的漏洞，并发现了4个新的npm包漏洞。

Conclusion: LLMs可以实际增强现有的静态程序分析算法，结合符号推理和语义理解，提高JavaScript漏洞检测的效果。

Abstract: Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.

</details>


### [5] [Adaptive Privacy Budgeting](https://arxiv.org/abs/2601.10866)
*Yuting Liang,Ke Yi*

Main category: cs.CR

TL;DR: 该论文研究了在广义差分隐私（GDP）下进行自适应隐私预算分配的问题，以优化后续查询的隐私保护和数据效用。


<details>
  <summary>Details</summary>
Motivation: 在多个查询场景中，不同用户的数据对隐私的影响不均衡，因此需要一种动态调整隐私预算的机制，以达到更好的隐私保护和数据效用。

Method: 提出一种自适应隐私预算分配框架，能够根据先前查询的输出调整后续查询的隐私预算。

Result: 该框架在多个应用场景下展示了其有效性和实用性，能够实现隐私节省并提高后续查询的效用。

Conclusion: 自适应隐私预算分配是一个重要的研究方向，有助于在保护用户隐私的同时提升数据查询的实用性。

Abstract: We study the problem of adaptive privacy budgeting under generalized differential privacy. Consider the setting where each user $i\in [n]$ holds a tuple $x_i\in U:=U_1\times \dotsb \times U_T$, where $x_i(l)\in U_l$ represents the $l$-th component of their data. For every $l\in [T]$ (or a subset), an untrusted analyst wishes to compute some $f_l(x_1(l),\dots,x_n(l))$, while respecting the privacy of each user. For many functions $f_l$, data from the users are not all equally important, and there is potential to use the privacy budgets of the users strategically, leading to privacy savings that can be used to improve the utility of later queries. In particular, the budgeting should be adaptive to the outputs of previous queries, so that greater savings can be achieved on more typical instances. In this paper, we provide such an adaptive budgeting framework, with various applications demonstrating its applicability.

</details>


### [6] [Towards Quantum-Resistant Trusted Computing: Architectures for Post-Quantum Integrity Verification Techniques](https://arxiv.org/abs/2601.11095)
*Grazia D'Onghia,Antonio Lioy*

Main category: cs.CR

TL;DR: 本文分析了现有信任技术在量子计算威胁下的局限性，并探讨了向后量子密码学（PQC）过渡的必要性，提出了结合PQC的信任计算架构。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机的发展，传统的基于非对称密码学的信任技术面临严重威胁，因此需要转向量子抗性更强的后量子密码学（PQC）来确保系统的安全性。

Method: 本文研究了当前常见的信任技术，并分析了它们向后量子密码学过渡的可行性。同时，从集成角度探讨了PQC在现有可信计算（TC）方案中的挑战，并提出了一种增强型TC架构。

Result: 研究结果显示了将现有可信计算系统与PQC结合的必要性和可行性，同时指出了这一过渡过程中可能遇到的技术挑战。

Conclusion: 本文强调了向后量子密码学过渡的紧迫性，并提出了一个结合PQC的增强型信任计算架构，为未来的安全系统提供了参考方向。

Abstract: Trust is the core building block of secure systems, and it is enforced through methods to ensure that a specific system is properly configured and works as expected. In this context, a Root of Trust (RoT) establishes a trusted environment, where both data and code are authenticated via a digital signature based on asymmetric cryptography, which is vulnerable to the threat posed by Quantum Computers (QCs). Firmware, being the first layer of trusted software, faces unique risks due to its longevity and difficult update. The transition of firmware protection to Post-Quantum Cryptography (PQC) is urgent, since it reduces the risk derived from exposing all computing and network devices to quantum-based attacks. This paper offers an analysis of the most common trust techniques and their roadmap towards a Post-Quantum (PQ) world, by investigating the current status of PQC and the challenges posed by such algorithms in existing Trusted Computing (TC) solutions from an integration perspective. Furthermore, this paper proposes an architecture for TC techniques enhanced with PEC, addressing the imperative for immediate adoption of quantum-resistant algorithms.

</details>


### [7] [Shaping a Quantum-Resistant Future: Strategies for Post-Quantum PKI](https://arxiv.org/abs/2601.11104)
*Grazia D'Onghia,Diana Gratiela Berbecaru,Antonio Lioy*

Main category: cs.CR

TL;DR: 本文探讨了在量子计算时代如何确保经典公钥密码协议的安全性，重点分析了选择稳健的后量子算法及其在公钥基础设施（PKI）中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统公钥密码可能面临被破解的风险，因此需要研究量子抗性算法以保障信息安全。

Method: 本文通过比较分析，研究量子抗性算法在PKI环境中的适用性，重点关注X.509证书格式的适应性，并探索证书撤消列表和在线证书状态协议的量子抗性改造。

Result: 本文提出了确保安全过渡到量子抗性PKI的要求，并探讨了相关技术的实施方法。

Conclusion: 本文强调了快速采用量子抗性算法的必要性，并为实现这一目标提供了指导性的框架和建议。

Abstract: As the quantum computing era approaches, securing classical cryptographic protocols becomes imperative. Public key cryptography is widely used for signature and key exchange but it is the type of cryptography more threatened by quantum computing. Its application typically requires support via a public-key certificate, which is a signed data structure and must therefore face twice the quantum challenge: for the certified keys and for the signature itself. We present the latest developments in selecting robust Post-Quantum algorithms and investigate their applicability in the Public Key Infrastructure context. Our contribution entails defining requirements for a secure transition to a quantum-resistant Public Key Infrastructure, with a focus on adaptations for the X.509 certificate format. Additionally, we explore transitioning Certificate Revocation List and Online Certificate Status Protocol to support quantum-resistant algorithms. Through comparative analysis, we elucidate the complex transition to a quantum-resistant PKI.

</details>


### [8] [Proving Circuit Functional Equivalence in Zero Knowledge](https://arxiv.org/abs/2601.11173)
*Sirui Shen,Zunchen Huang,Chenglu Jin*

Main category: cs.CR

TL;DR: 本文提出了一种名为ZK-CEC的隐私保护硬件形式验证框架，结合了形式验证和零知识证明技术，能够在不泄露设计细节的情况下验证IP模块的正确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着现代集成电路越来越多地依赖第三方IP集成，安全风险也随之增加，包括硬件木马和安全漏洞。现有的隐私保护硬件验证方法基于模拟，无法提供形式保证，因此需要一种新的隐私保护形式验证技术。

Method: 提出了一种用于证明秘密设计对公共约束的不可满足性的蓝图，并基于该蓝图构建了ZK-CEC框架，使得验证者能够确认秘密IP的功能完全符合公共规范，且仅泄露证明的长度和宽度。

Result: 在实际电路（如算术单元和加密组件）上的实验结果表明，ZK-CEC能够在合理的时间内成功验证实际设计，如AES S-Box。

Conclusion: ZK-CEC为在不暴露设计机密的前提下形式验证IP的正确性和安全性奠定了基础，解决了现有隐私保护验证方法在形式保证方面的不足。

Abstract: The modern integrated circuit ecosystem is increasingly reliant on third-party intellectual property integration, which introduces security risks, including hardware Trojans and security vulnerabilities. Addressing the resulting trust deadlock between IP vendors and system integrators without exposing proprietary designs requires novel privacy-preserving verification techniques. However, existing privacy-preserving hardware verification methods are all simulation-based and fail to offer formal guarantees. In this paper, we propose ZK-CEC, the first privacy-preserving framework for hardware formal verification. By combining formal verification and zero-knowledge proof (ZKP), ZK-CEC establishes a foundation for formally verifying IP correctness and security without compromising the confidentiality of the designs.
  We observe that existing zero-knowledge protocols for formal verification are designed to prove statements of public formulas. However, in a privacy-preserving verification context where the formula is secret, these protocols cannot prevent a malicious prover from forging the formula, thereby compromising the soundness of the verification. To address these gaps, we first propose a blueprint for proving the unsatisfiability of a secret design against a public constraint, which is widely applicable to proving properties in software, hardware, and cyber-physical systems. Based on the proposed blueprint, we construct ZK-CEC, which enables a prover to convince the verifier that a secret IP's functionality aligns perfectly with the public specification in zero knowledge, revealing only the length and width of the proof. We implement ZK-CEC and evaluate its performance across various circuits, including arithmetic units and cryptographic components. Experimental results show that ZK-CEC successfully verifies practical designs, such as the AES S-Box, within practical time limits.

</details>


### [9] [SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11199)
*Aiman Al Masoud,Marco Arazzi,Antonino Nocera*

Main category: cs.CR

TL;DR: 本文提出了一种名为 SD-RAG 的新型方法，用于在 Retrieval-Augmented Generation 中实现选择性披露，以提高隐私和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 方法忽略了在生成过程中暴露敏感信息的风险，且容易受到提示注入攻击的影响，因此需要一种更安全的方法来处理隐私和安全约束。

Method: SD-RAG 通过在检索阶段应用数据清洗和披露控制，将安全约束与生成过程分离，并引入语义机制和优化的图数据模型，以支持基于策略的精细检索。

Result: 实验结果表明，SD-RAG 在隐私评分上比现有方法提高了最多 58%，并且对针对生成模型的提示注入攻击具有较强的抵御能力。

Conclusion: SD-RAG 为 RAG 提供了一种更安全的选择性披露方案，有效降低了隐私和安全风险。

Abstract: Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.

</details>


### [10] [InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers](https://arxiv.org/abs/2601.11368)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: 该论文提出了一种名为InterPUF的认证框架，用于解决现代SiP平台中重新配置互连器带来的信任挑战。InterPUF通过将互连器作为分布式根信任，并使用多方计算确保原始PUF签名不被暴露，从而在异构多供应商生态系统中实现安全的插拔式芯片集成。


<details>
  <summary>Details</summary>
Motivation: 现代系统封装（SiP）平台采用可重新配置的互连器以实现插拔式芯片集成，但这种灵活性带来了严重的信任问题，传统认证方案无法在去中心化、可编程的后制造环境中扩展和适应。

Method: InterPUF嵌入了基于路由的差异延迟物理不可克隆函数（PUF）到可重新配置的互连器中，并利用多方计算（MPC）进行认证，确保原始PUF签名不会被暴露。认证过程结合了互连器内部的PUF特性、密码哈希和协作验证。

Result: 硬件评估显示，InterPUF在不同芯片上仅产生0.23%的面积和0.072%的功耗开销，同时保持认证延迟在十纳秒以内。通过pyPUF的模拟结果，证实了InterPUF在工艺、电压和温度变化下的独特性、可靠性和建模抵抗力。

Conclusion: InterPUF通过将互连器作为分布式根信任，结合PUF、密码哈希和协作验证，实现了无需集中锚点的最小信任认证模型，为SiP平台提供了安全、可扩展的解决方案。

Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.

</details>


### [11] [Understanding Help Seeking for Digital Privacy, Safety, and Security](https://arxiv.org/abs/2601.11398)
*Kurt Thomas,Sai Teja Peddinti,Sarah Meiklejohn,Tara Matthews,Amelia Hassoun,Animesh Srivastava,Jessica McClearn,Patrick Gage Kelley,Sunny Consolvo,Nina Taft*

Main category: cs.CR

TL;DR: 该研究通过分析Reddit上的10亿条帖子，识别用户寻求数字隐私、安全和保密帮助的模式，并构建了一个高精度的专题数据集。


<details>
  <summary>Details</summary>
Motivation: 用户在面对数字隐私、安全和保密威胁时经常寻求帮助，因此需要更好地理解用户在实践中如何寻求帮助，以改进相关资源。

Method: 研究结合定性编码与大型语言模型的微调，筛选出过去四年Reddit中与数字隐私、安全和保密相关的话题，并自动标注每一个帖子的讨论主题。

Result: 研究确定了三百万条相关帖子，精度和召回率分别为93%，并发现了用户寻求帮助的范围、社区分布和帮助类型。

Conclusion: 这项研究为用户资源（如用户指南或帮助助手）的发展提供了重要信息，同时也突显了支持用户应对复杂威胁和平台挑战的困难。

Abstract: The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）如何编码在线信息中的信任感，并利用PEACE-Reviews数据集分析不同模型在信任线索上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在搜索、推荐和对话系统中的广泛应用，其是否能以心理上一致的方式表示信任感成为一个关键问题。

Method: 使用PEACE-Reviews数据集，通过分析LLMs的层和头激活差异，研究其在编码信任线索方面的表现，并进行探针分析以验证信任信号的可解码性。

Result: 不同LLMs在编码信任线索上存在系统性的差异，且信任信号在预训练阶段已隐含编码。公平性、确定性和自我问责性等维度与信任形成有最强关联。

Conclusion: 现代LLMs能够无监督地内部化心理基础的信任信号，为构建可信赖、透明的AI系统提供了表示基础。

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [13] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA 是一种解决多时间尺度智能体架构中协调不稳定问题的框架，通过引入三种约束机制提高系统的稳定性和决策效率。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但其协调稳定性问题导致了层间冲突、误差传播和可扩展性受限。

Method: CTHA 通过将层间通信空间投影到结构化流形上，结合信息流约束、决策空间约束和仲裁机制，实现协调稳定与决策一致。

Result: 实验表明，CTHA 在复杂任务执行中有效，可减少 47% 的失败级联，提高 2.3 倍的样本效率，并优于未约束的分层基线。

Conclusion: CTHA 作为时间分层架构的有原则扩展，有助于深入理解多智能体协调，并推动稳健自主系统的演进。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [14] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 本文探讨了复杂产品创新过程，使用基于一组启发式的模型，利用简单的趋势（增加、减少或恒定）作为信息量最小的量化指标，避免使用数值或粗糙集。


<details>
  <summary>Details</summary>
Motivation: 研究复杂产品创新过程的目的是通过简化模型来更有效地分析系统的行为，同时减少对复杂数据的依赖。

Method: 通过定义趋势模型的解决方案为具有可能转换的场景集，并用转移图表示这些转换关系，从而描绘系统的可能未来或过去行为。

Result: 这种方法能够以较低的信息复杂度捕捉系统的动态变化，并提供可视化路径来表示系统的可能演变。

Conclusion: 利用趋势模型和转移图能够有效描述复杂产品创新过程的行为，并为系统分析提供一个新的视角。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [15] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: 本文分析了ARC-AGI基准系列在评估少样本任务泛化能力中的重要性，并讨论了当前AI推理性能在知识覆盖上的限制，同时展望了ARC-AGI-3的新挑战。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准系列旨在衡量AI在新任务中的少样本泛化能力，这是实现通用人工智能（AGI）的关键方面。随着ARC-AGI-2数据集的发布，研究兴趣和方法的多样性显著增加，但现有方法仍面临知识覆盖的局限性。

Method: 本文调研了表现优异的方法，分析了精炼循环在AGI进展中的作用，并探讨了知识依赖的过拟合问题。此外，本文还介绍了ARC-AGI-3，它引入了需要探索、规划、记忆、目标获取和对齐能力的交互推理挑战。

Result: 分析表明，当前前沿AI的推理性能仍受知识覆盖的限制，导致新的基准污染形式。ARC-AGI-2数据集的竞赛吸引了大量团队和提交，但最高认证得分仅达到24%。

Conclusion: 尽管精炼循环方法和零预训练深度学习方法在ARC-AGI-2中表现出一定的竞争力，但知识依赖的过拟合仍限制了推理效果。ARC-AGI-3将引入更具挑战性的交互推理任务，推动AI在更复杂的推理能力上的发展。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [16] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 本研究通过NeurIPS 2025的DCVLR挑战，分析了多模态推理中的数据筛选对模型性能的影响，并发现难度驱动的例子选择是主要的性能提升因素。


<details>
  <summary>Details</summary>
Motivation: 为了提高多模态推理的效率，研究者希望通过数据筛选优化模型表现，特别是在固定模型和训练协议的情况下寻找有效的数据策略。

Method: 研究使用了一个基于 Walton Multimodal Cold Start 的精简数据集，并通过后竞赛的消融实验验证了难度驱动的例子选择对性能的影响。

Result: 实验结果显示，难度驱动的例子选择显著提升了模型性能，而增加数据集大小对平均准确率的提升不明显，反而降低了运行间的方差；常用的多样性增广方法没有带来额外的提升，反而可能降低性能。

Conclusion: DCVLR挑战可以被视为一个饱和阶段评估，强调了数据对齐和例子难度在提升多模态推理效率中的核心作用。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [17] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: HADES is a Bayesian optimization method for engineering optimized protein variants by integrating protein structure and sequence constraints.


<details>
  <summary>Details</summary>
Motivation: Existing sequence-based optimization methods face challenges due to high-dimensional complexities and lack of structural awareness.

Method: HADES employs a structure-aware approximated posterior through a two-stage encoder-decoder framework and uses Hamiltonian dynamics for efficient sampling. It also introduces a position discretization procedure to generate discrete sequences.

Result: Extensive experiments show HADES outperforms state-of-the-art methods in in-silico evaluations across most metrics.

Conclusion: HADES provides a novel approach for protein variant design by effectively leveraging the mutual constraints between structure and sequence, leading to optimized properties.

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [18] [MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting](https://arxiv.org/abs/2601.11089)
*Suhan Guo,Jiahong Deng,Furao Shen*

Main category: cs.AI

TL;DR: 本文提出了一种名为MiCA的轻量级模块用于传染病预测，通过因果发现推断移动关系，并将其整合到时间预测模型中，从而提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于传染病动态预测对公共卫生活动至关重要，但人类移动数据和疾病记录的整合存在噪声、间接性和数据稀缺性等问题，因此需要一种更轻量且有效的预测方法。

Method: MiCA模块通过因果发现推断人类移动关系，并使用门控残差混合机制将这些移动关系整合到时间预测模型中，从而在无需复杂关系组件的情况下提升模型表现。

Result: 在四个真实世界传染病数据集上的实验表明，MiCA显著提高了轻量级时间模型的预测性能，平均相对误差减少了7.5%。此外，其性能与最先进的时空模型竞争。

Conclusion: MiCA提供了一种轻量、架构独立的方法，在保持模型简洁的同时有效提高了传染病预测的准确性，尤其适合数据有限或噪声较大的情况。

Abstract: Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.

</details>


### [19] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本研究提出了一种名为SCALE的低成本任务级生成框架，用于优化多智能体系统中的工作流生成，通过较少的校准代替全面验证，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统工作流生成方法在任务或查询级别进行生成，但其成本和效益尚不明确，需要更高效的解决方案。

Method: 基于自进化和生成性奖励建模理念，提出一种使用少量样本进行优化器自预测的校准方法，从而在任务级进行工作流生成。

Result: 实验表明，SCALE在多个数据集上保持了与现有方法相当的性能，平均仅下降0.61%，同时将整体的token使用量减少了高达83%。

Conclusion: SCALE能够在减少计算成本的情况下，有效提升多智能体系统工作流生成的效率和性能，适用于实际应用场景。

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [20] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: Think-with-Me 是一种新型的测试时交互推理方法，通过在推理过程中引入外部反馈干预来提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多步推理中效率低下，存在过度思考和冗余推理的问题，导致计算成本高且性能下降。

Method: 通过在推理的过渡连接词处暂停，并利用多标准评估（理性与完整性）获取外部反馈，以动态延长或终止推理。

Result: 在 AIME24 数据集上，与 QwQ-32B 相比，准确率提高了 7.19%，推理长度减少了 81%。

Conclusion: Think-with-Me 能有效平衡准确率与推理长度，并适用于安全和创造性任务。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [21] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是用于评估AI与人类对齐的可解释框架，通过机制模型分析人类和LLM决策中的可解释参数来诊断和改进AI系统的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AI对齐评估方法依赖于表面结果的一致性，如准确性或F1分数，无法揭示深层决策机制是否与人类一致。XChoice框架旨在提供更细致的评估方法，以识别AI系统与人类决策过程中的对齐偏差。

Method: XChoice通过拟合基于机制的决策模型来分析人类数据和大语言模型（LLM）生成的决策，提取可解释参数，包括决策因素的重要性、约束敏感性以及隐含的权衡关系。通过比较不同模型、选项和子群体的参数向量，评估AI与人类的对齐程度。

Result: 实验表明，XChoice在评估美国人的日常时间分配时，成功揭示了AI模型与人类决策在不同活动和群体中的异质性对齐情况，并发现了在Black和婚姻状态为已婚的群体中存在显著的对齐偏差。

Conclusion: XChoice提供了基于机制的对齐评估指标，有助于深入诊断AI系统的对齐问题，并支持更加有针对性的改进措施。该框架超越了单纯的表面结果匹配，提供了关于AI决策行为的更详细分析。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [22] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 本文介绍了一种名为probe and solve的算法，用于自动化优化约束编程求解器的超参数，通过两个阶段（探查和求解）提升求解性能。


<details>
  <summary>Details</summary>
Motivation: 约束编程求解器的性能高度依赖于超参数的选择，手动寻找最佳配置困难且耗时，需要专家知识，因此需要一种自动化的方法。

Method: 该方法整合到CPMpy库中，分为探查阶段和求解阶段。探查阶段使用可配置的超参数优化方法（如贝叶斯优化和汉明距离搜索）来探索不同超参数设置，求解阶段使用最优配置解决实际问题。

Result: 实验结果显示，贝叶斯优化比默认配置在ACE求解器中表现出色，改进了25.4%的实例的求解质量，在Choco求解器中也有38.6%的实例表现更优。同时，贝叶斯优化优于汉明距离搜索，证明了基于模型的探索优于简单的局部搜索。

Conclusion: 该算法为约束求解器的超参数调优提供了一种实用且资源感知的方法，显著提升了求解质量并适用于多种问题类型。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [23] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于大型语言模型（LLM）的预测过程监控框架，通过在多个关键绩效指标（KPIs）上进行评估，表明在数据稀缺的情况下，LLM在预测过程中优于传统基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测过程监控旨在预测正在进行的过程的未来结果，而LLM因其在理解和推断复杂模式方面的潜力，被用于这一领域。然而，其泛化能力、语义利用和推理机制尚未得到充分验证。

Method: 本文扩展了基于LLM的预测框架，通过在三个不同的事件日志上评估总时间及活动发生预测等KPIs，分析LLM的泛化能力、语义利用和推理机制。

Result: 在只有100个轨迹的数据稀缺场景中，LLM在总时间及活动发生预测等KPIs上优于传统基准方法，并且利用其内置的先验知识和训练轨迹之间的内部相关性。

Conclusion: LLM不仅能够有效利用其已有的知识，还能在不同KPIs上进行高阶推理，使其在预测过程监控中展现出更强大的性能。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [24] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 本文提出一种混合框架LEG，结合专家知识和优化技术，以提高埃塞俄比亚农村地区的卫生服务覆盖率。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部正在升级卫生站以提高基本医疗服务的可及性，但有限的资源需要仔细优先选择升级的设施，以最大化人口覆盖率并考虑不同专家和利益相关者的偏好。

Method: 本文提出的LEG框架结合了可证明近似算法与大型语言模型（LLM）驱动的迭代优化方法，以整合专家的定性指导和优化模型的理论保证。

Result: 在埃塞俄比亚三个地区的实验结果表明，该框架有效提高了卫生服务的覆盖率，并展示了其在公平、数据驱动的卫生系统规划中的潜力。

Conclusion: 该混合框架LEG能够在考虑专家意见和利益相关者偏好基础上，实现卫生资源配置的优化，并为公共卫生决策提供有力支持。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>
